{
    "SST2":{
        "devacc":67.2,
        "ntest":1821,
        "acc":67.11,
        "ndev":872
    },
    "CR":{
        "devacc":70.64,
        "ntest":3775,
        "acc":70.65,
        "ndev":3775
    },
    "MRPC":{
        "devacc":70.61,
        "ntest":1725,
        "acc":70.96,
        "f1":80.1,
        "ndev":4076
    },
    "MR":{
        "devacc":64.53,
        "ntest":10662,
        "acc":64.72,
        "ndev":10662
    },
    "STS14":{
        "deft-news":{
            "spearman":[
                0.578333454614915,
                3.56018796171822e-28
            ],
            "pearson": [
                0.6220449533709097,
                1.6081763124275315e-33
            ],
            "nsamples":300
        },
        "headlines":{
            "spearman":[
                0.5705232857978165,
                5.589791453157285e-66
            ],
            "pearson":[
                0.5781994919225272,
                4.020000246168714e-68
            ],
            "nsamples":750
        },
        "images":{
            "spearman":[
                0.6438621123340441,
                4.9441358581162626e-89
            ],
            "pearson":[
                0.6578237117176955,
                3.654157740608177e-94
            ],
            "nsamples":750
        },
        "tweet-news":{
            "spearman":[
                0.552714320253843,
                3.22156860724007e-61
            ],
            "pearson":[
                0.6233064013917692,
                6.006549841321549e-82
            ],
            "nsamples":750
        },
        "OnWN":{
            "spearman":[
                0.7165302641879749,
                4.038459846586195e-119
            ],
            "pearson":[
                0.658846051664604,
                1.5004930667755487e-94
            ],
            "nsamples":750
        },
        "all":{
            "spearman":{
                "wmean":0.5865540558636717,
                "mean":0.5708291603366306
            },
            "pearson":{
                "wmean":0.5951356658320022,
                "mean":0.5813380714320986
            }
        },
        "deft-forum":{
            "spearman":[
                0.3630115248311902,
                1.833847351421204e-15
            ],
            "pearson": [
                0.3478078185250862,
                3.053271390924939e-14
            ],
            "nsamples":450
        }
    },
    "MPQA":{
        "devacc":83.89,
        "ntest":10606,
        "acc":83.78,
        "ndev":10606
    },
    "TREC":{
        "devacc":53.56,
        "ntest":500,
        "acc":57.4,
        "ndev":5452
    },
    "SICKEntailment":{
        "devacc":72.6,
        "ntest":4927,
        "acc":74.75,
        "ndev":500
    },
    "SUBJ":{
        "devacc":79.17,
        "ntest":10000,
        "acc":79.15,
        "ndev":10000
    }
}
