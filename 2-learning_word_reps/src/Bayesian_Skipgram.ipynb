{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_data(text_file):\n",
    "\n",
    "    data_output = []\n",
    "    with open(text_file, \"r\") as file1:\n",
    "        data_list = file1.readlines()\n",
    "    for line in data_list:\n",
    "        token_list = line.split()\n",
    "        data_output.append(token_list)\n",
    "    \n",
    "    return data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_in_data(\"../data/english-french_small/dev.en\")\n",
    "\n",
    "training_data = training_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess data\n",
    "\n",
    "MAX_VOCABULARY_SIZE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(training_set):\n",
    "\n",
    "    vocabulary = []\n",
    "    for sentence in training_set:\n",
    "        for word in sentence:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "                \n",
    "    w2i = dict()\n",
    "    i2w = dict()\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        i2w[idx] = word\n",
    "        w2i[word] = idx\n",
    "    \n",
    "    return w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, i2w = create_vocabulary(training_data)\n",
    "V = len(w2i.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram(sentence, context_window_size):\n",
    "\n",
    "    skipgram_array = []\n",
    "    for idx, word in enumerate(sentence):\n",
    "        context_set = []\n",
    "        window_size = context_window_size\n",
    "        for index in range(max(idx - window_size, 0), min(len(sentence), idx + window_size + 1)):\n",
    "            if index != idx:\n",
    "                context_set.append([word, sentence[index]])\n",
    "        skipgram_array.append(context_set)\n",
    "\n",
    "    return skipgram_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "# Training data :\n",
    "context_data = [generate_skipgram(sentence, WINDOW_SIZE) for sentence in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_batches(training_set, batch_size):\n",
    "\n",
    "    sentences = training_set\n",
    "    random.shuffle(sentences)\n",
    "    \n",
    "    new_data = []\n",
    "    num_samples = len(sentences)\n",
    "    for idx in range(num_samples // batch_size):\n",
    "        batch = sentences[(idx)*batch_size : (idx+1)*batch_size]\n",
    "        new_data.append(batch)\n",
    "    \n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(word, vocab_size=V):\n",
    "    one_hot = torch.zeros(vocab_size)\n",
    "    one_hot[w2i[word]] = 1.0\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "input_data = make_input_batches(training_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergence_closed_form(mu, variance):\n",
    "    '''\n",
    "    Closed form of the KL divergence\n",
    "    '''\n",
    "    return -0.5 * torch.sum(1 + variance - torch.pow(mu, 2) - torch.exp(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO():\n",
    "    '''\n",
    "    Evidence Lower BOund\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearity(nn.Module):\n",
    "    def __init__(self, embedding_dimension, vocabulary_size):\n",
    "        super(linearity, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSG_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dimension=20):\n",
    "\n",
    "        super(BSG_Net, self).__init__()\n",
    "\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.fc1 = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "        self.fc2 = nn.Linear(embedding_dimension * 2, embedding_dimension * 2)\n",
    "        self.fc3 = nn.Linear(embedding_dimension * 2, embedding_dimension)\n",
    "        self.fc4 = nn.Linear(embedding_dimension * 2, embedding_dimension)\n",
    "        \n",
    "        # for reparameterization\n",
    "        self.re1 = nn.Linear(embedding_dimension, vocabulary_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        context_representation = torch.zeros(self.embedding_dimension * 2)\n",
    "\n",
    "        for pair in x:\n",
    "            center_word = self.fc1(onehot(pair[0]))\n",
    "            context_word = self.fc1(onehot(pair[1]))\n",
    "\n",
    "            concatenated = torch.cat([center_word, context_word], dim=0)\n",
    "            concatenated = F.relu(self.fc2(concatenated))\n",
    "            context_representation += concatenated\n",
    "\n",
    "        mu = self.fc3(context_representation)\n",
    "        sigma = F.softplus(self.fc4(context_representation))\n",
    "\n",
    "        epsilon = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(self.embedding_dimension), torch.eye(self.embedding_dimension)).sample()\n",
    "\n",
    "        z = mu + epsilon * sigma\n",
    "\n",
    "        output = F.softmax(self.re1(z), dim=0)\n",
    "    \n",
    "        return output, mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dimension=20):\n",
    "\n",
    "        super(prior_Net, self).__init__()\n",
    "\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.L = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "        self.S = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dimension, vocabulary_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        one_hot_x = onehot(x)\n",
    "        print(one_hot_x)\n",
    "\n",
    "        mean = self.L(one_hot_x)\n",
    "        print(mean)\n",
    "        std = F.softplus(self.S(one_hot_x))\n",
    "        print(std)\n",
    "        \n",
    "        variance = torch.diag(std ** 2)\n",
    "        print(variance.size())\n",
    "        \n",
    "        z = torch.distributions.multivariate_normal.MultivariateNormal(mean,variance).sample()\n",
    "\n",
    "        return F.softmax(self.fc1(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.])\n",
      "tensor([ 0.1105,  0.0659,  0.0550,  0.0430, -0.0277,  0.0152,  0.0858,\n",
      "        -0.0045, -0.1165, -0.0768, -0.0380,  0.1864,  0.0444,  0.0791,\n",
      "        -0.1116, -0.0209,  0.0007, -0.0714,  0.1654, -0.0790])\n",
      "tensor([ 0.6355,  0.7289,  0.6768,  0.7013,  0.6903,  0.7614,  0.6969,\n",
      "         0.6809,  0.6958,  0.6241,  0.7587,  0.7732,  0.6859,  0.6803,\n",
      "         0.7058,  0.7029,  0.7312,  0.6305,  0.6773,  0.7072])\n",
      "torch.Size([20, 20])\n",
      "tensor(1.00000e-02 *\n",
      "       [ 1.5288,  1.5307,  1.7900,  2.0366,  1.0883,  2.0240,  1.4070,\n",
      "         0.9914,  0.6503,  1.0951,  1.6566,  1.3725,  1.3516,  1.0975,\n",
      "         0.9637,  1.5371,  1.0965,  1.1419,  1.3495,  0.8860,  0.6961,\n",
      "         1.4046,  1.0942,  1.4295,  2.3060,  1.1046,  1.3298,  1.4331,\n",
      "         1.5419,  1.4771,  1.0708,  1.0236,  1.1522,  1.0889,  0.7497,\n",
      "         1.1369,  0.9468,  1.1188,  0.6848,  0.9735,  0.9726,  0.9902,\n",
      "         0.6106,  1.1216,  2.7263,  1.3136,  2.0313,  1.2044,  3.0062,\n",
      "         1.4166,  1.4530,  0.6980,  1.3184,  1.2687,  0.9963,  0.6779,\n",
      "         1.7281,  1.6112,  1.7992,  1.2236,  1.3299,  1.3458,  0.9377,\n",
      "         1.5983,  1.0774,  1.2932,  2.6550,  1.8455,  1.6474,  1.6896,\n",
      "         2.4574,  1.7725,  0.9735,  0.8508])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "model = prior_Net(V, 20)\n",
    "\n",
    "X_data = []\n",
    "\n",
    "for sentence in training_data:\n",
    "    for context_set in sentence:\n",
    "        print(context_set)\n",
    "        print(model(context_set))\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each ['of', 'them']\n",
      "(tensor(1.00000e-02 *\n",
      "       [ 1.1333,  0.3253,  0.7358,  0.8131,  0.4344,  0.9877,  1.7538,\n",
      "         1.2249,  1.7645,  1.5386,  1.7679,  0.4074,  0.9355,  1.2049,\n",
      "         0.4938,  1.5430,  4.2881,  0.9575,  1.0323,  1.3115,  1.0283,\n",
      "         0.9543,  1.1669,  0.8652,  0.4009,  0.8702,  0.4023,  1.8159,\n",
      "         1.4200,  1.2375,  0.6987,  0.8913,  0.7348,  1.2793,  0.7638,\n",
      "         0.9496,  1.1159,  0.6792,  1.3865,  1.9624,  2.3016,  3.2830,\n",
      "         1.5433,  0.3706,  1.5974,  1.5622,  1.7638,  1.1937,  3.3319,\n",
      "         0.8813,  1.1449,  3.3667,  1.0649,  1.8419,  2.5309,  1.6430,\n",
      "         2.1641,  0.9611,  2.0965,  1.2293,  2.9615,  0.8344,  1.1256,\n",
      "         0.6994,  2.7630,  0.7990,  0.8209,  1.9170,  1.4329,  0.5622,\n",
      "         1.0094,  0.5617,  0.9119,  2.4526]), tensor([ 0.2179, -0.1650,  0.0678,  0.0549,  0.2392, -0.1790,  0.0595,\n",
      "         0.3243,  0.0840,  0.0212,  0.1459, -0.0770,  0.0488, -0.0572,\n",
      "         0.1568,  0.0681,  0.0598, -0.3085,  0.0341, -0.3099]), tensor([ 0.6451,  0.5869,  0.7317,  0.6783,  0.5192,  0.8022,  0.5739,\n",
      "         0.7084,  0.6433,  0.7511,  0.7530,  0.8502,  0.5737,  0.7545,\n",
      "         0.5595,  0.6487,  0.7464,  0.5928,  0.7133,  0.6119]))\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION = 20\n",
    "\n",
    "model = BSG_Net(V, EMBEDDING_DIMENSION)\n",
    "X_data = []\n",
    "\n",
    "for sentence in context_data:\n",
    "    for context_set in sentence:\n",
    "        centre_word = context_set[0][0]\n",
    "        context_words = []\n",
    "        for pair in context_set:\n",
    "            context_words.append(pair[1])\n",
    "        \n",
    "        print(centre_word, context_words)\n",
    "        print(model(context_set))\n",
    "        break\n",
    "    break\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for context_set in sentence:\n",
    "#         to_add = []\n",
    "#         for pair in context_set:\n",
    "#             center_word = model_1(onehot(pair[0]))\n",
    "#             context_word = model_1(onehot(pair[1]))\n",
    "\n",
    "#             concatenated = torch.cat([center_word, context_word], dim=0)\n",
    "#             concatenated = F.relu(model_2(concatenated))\n",
    "#             to_add.append(concatenated)\n",
    "            \n",
    "#         X_data.append(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
