{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_data(text_file):\n",
    "\n",
    "    data_output = []\n",
    "    with open(text_file, \"r\") as file1:\n",
    "        data_list = file1.readlines()\n",
    "    for line in data_list:\n",
    "        token_list = line.split()\n",
    "        data_output.append(token_list)\n",
    "    \n",
    "    return data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_in_data(\"../data/english-french_small/dev.en\")\n",
    "\n",
    "training_data = training_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess data\n",
    "\n",
    "MAX_VOCABULARY_SIZE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(training_set):\n",
    "\n",
    "    vocabulary = []\n",
    "    for sentence in training_set:\n",
    "        for word in sentence:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "                \n",
    "    w2i = dict()\n",
    "    i2w = dict()\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        i2w[idx] = word\n",
    "        w2i[word] = idx\n",
    "    \n",
    "    return w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, i2w = create_vocabulary(training_data)\n",
    "V = len(w2i.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram(sentence, context_window_size):\n",
    "\n",
    "    skipgram_array = []\n",
    "    for idx, word in enumerate(sentence):\n",
    "        context_set = []\n",
    "        window_size = context_window_size\n",
    "        for index in range(max(idx - window_size, 0), min(len(sentence), idx + window_size + 1)):\n",
    "            if index != idx:\n",
    "                context_set.append([word, sentence[index]])\n",
    "        skipgram_array.append(context_set)\n",
    "\n",
    "    return skipgram_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "# Training data :\n",
    "context_data = [generate_skipgram(sentence, WINDOW_SIZE) for sentence in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_batches(training_set, batch_size):\n",
    "\n",
    "    sentences = training_set\n",
    "    random.shuffle(sentences)\n",
    "    \n",
    "    new_data = []\n",
    "    num_samples = len(sentences)\n",
    "    for idx in range(num_samples // batch_size):\n",
    "        batch = sentences[(idx)*batch_size : (idx+1)*batch_size]\n",
    "        new_data.append(batch)\n",
    "    \n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(word, vocab_size=V):\n",
    "    one_hot = torch.zeros(vocab_size)\n",
    "    one_hot[w2i[word]] = 1.0\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "input_data = make_input_batches(training_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergence_closed_form(mu, variance):\n",
    "    '''\n",
    "    Closed form of the KL divergence\n",
    "    '''\n",
    "    return -0.5 * torch.sum(1 + variance - torch.pow(mu, 2) - torch.exp(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO():\n",
    "    '''\n",
    "    Evidence Lower BOund\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearity(nn.Module):\n",
    "    def __init__(self, embedding_dimension, vocabulary_size):\n",
    "        super(linearity, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSG_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dimension=20):\n",
    "\n",
    "        super(BSG_Net, self).__init__()\n",
    "\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.fc1 = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "        self.fc2 = nn.Linear(embedding_dimension * 2, embedding_dimension * 2)\n",
    "        self.fc3 = nn.Linear(embedding_dimension * 2, embedding_dimension)\n",
    "        self.fc4 = nn.Linear(embedding_dimension * 2, embedding_dimension)\n",
    "        \n",
    "        # for reparameterization\n",
    "        self.re1 = nn.Linear(embedding_dimension, vocabulary_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        context_representation = torch.zeros(self.embedding_dimension * 2)\n",
    "\n",
    "        for pair in x:\n",
    "            center_word = self.fc1(onehot(pair[0]))\n",
    "            context_word = self.fc1(onehot(pair[1]))\n",
    "\n",
    "            concatenated = torch.cat([center_word, context_word], dim=0)\n",
    "            concatenated = F.relu(self.fc2(concatenated))\n",
    "            context_representation += concatenated\n",
    "\n",
    "        mu = self.fc3(context_representation)\n",
    "        sigma = F.softplus(self.fc4(context_representation))\n",
    "\n",
    "        epsilon = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(self.embedding_dimension), torch.eye(self.embedding_dimension)).sample()\n",
    "\n",
    "        z = mu + epsilon * sigma\n",
    "\n",
    "        output = F.softmax(self.re1(z), dim=0)\n",
    "    \n",
    "        return output, mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dimension=20):\n",
    "\n",
    "        super(prior_Net, self).__init__()\n",
    "\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.L = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "        self.S = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dimension, vocabulary_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        one_hot_x = onehot(x)\n",
    "\n",
    "        mean = self.L(one_hot_x)\n",
    "        std = F.softplus(self.S(one_hot_x))\n",
    "        z = torch.distributions.multivariate_normal.MultivariateNormal(mean,torch.mm(torch.eye(self.embedding_dimension), std)).sample()\n",
    "\n",
    "        return F.softmax(self.fc1(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2d748a2143e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcontext_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml2labs/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-b1d79b4bbfd4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "model = prior_Net(V, 20)\n",
    "\n",
    "X_data = []\n",
    "\n",
    "for sentence in training_data:\n",
    "    for context_set in sentence:\n",
    "        print(model(context_set))\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.00000e-02 *\n",
      "       [ 2.5028,  0.9399,  3.0797,  1.1794,  1.5582,  0.9494,  1.3026,\n",
      "         0.7748,  2.1435,  1.0236,  1.1902,  1.1210,  0.8714,  3.0826,\n",
      "         1.5120,  2.0029,  1.0275,  1.4397,  1.3446,  1.4108,  1.6099,\n",
      "         0.8656,  0.4063,  0.8884,  0.8358,  1.0510,  0.7438,  0.7721,\n",
      "         1.1343,  3.1821,  1.6474,  1.1428,  1.0547,  0.8487,  2.2437,\n",
      "         1.8391,  0.7768,  1.7111,  1.5330,  0.5382,  1.0233,  1.4370,\n",
      "         2.2370,  1.3191,  1.4335,  1.5465,  0.7196,  2.0937,  1.5601,\n",
      "         0.8138,  0.9269,  1.6357,  1.1205,  1.0677,  1.8069,  0.7476,\n",
      "         1.8372,  1.8415,  0.8729,  1.5163,  1.1594,  1.4797,  0.7273,\n",
      "         2.6367,  1.0206,  1.0036,  1.4164,  1.6321,  0.9004,  0.4773,\n",
      "         1.5247,  1.2849,  0.5629,  1.3362]), tensor([ 0.0628,  0.0070, -0.0653,  0.0863,  0.0874,  0.0363, -0.0386,\n",
      "        -0.1624, -0.0034,  0.3828, -0.0785,  0.3049, -0.1053, -0.0991,\n",
      "        -0.0387,  0.1531, -0.1124,  0.2176, -0.1926, -0.0540]), tensor([ 0.6794,  0.6923,  0.6006,  0.6767,  0.6850,  0.5828,  0.6072,\n",
      "         0.6805,  0.6411,  0.8475,  0.6338,  0.6206,  0.6770,  0.7877,\n",
      "         0.7158,  0.6810,  0.6346,  0.6834,  0.7235,  0.7822]))\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION = 20\n",
    "\n",
    "model = BSG_Net(V, EMBEDDING_DIMENSION)\n",
    "X_data = []\n",
    "\n",
    "for sentence in context_data:\n",
    "    for context_set in sentence:\n",
    "        print(model(context_set))\n",
    "        break\n",
    "    break\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for context_set in sentence:\n",
    "#         to_add = []\n",
    "#         for pair in context_set:\n",
    "#             center_word = model_1(onehot(pair[0]))\n",
    "#             context_word = model_1(onehot(pair[1]))\n",
    "\n",
    "#             concatenated = torch.cat([center_word, context_word], dim=0)\n",
    "#             concatenated = F.relu(model_2(concatenated))\n",
    "#             to_add.append(concatenated)\n",
    "            \n",
    "#         X_data.append(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
