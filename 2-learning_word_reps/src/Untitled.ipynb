{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_set = set()\n",
    "# for line in lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Embed_Align_Net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/embed_align-5001.model', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/embed_align-5001.torch-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/english-french_large/training-30000.en', 'r') as f:\n",
    "    full_text = f.read().splitlines()\n",
    "    \n",
    "for line in full_text:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        words_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1 ,0, 1.0])\n",
    "x = x.repeat(2, 1)\n",
    "\n",
    "y = torch.tensor([[1, -2, 3.0], [4, 4, 4.0]])\n",
    "\n",
    "z = torch.cat([x, y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0] * 4\n",
    "torch.LongTensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "zz = F.relu(z).sum(dim = 0)\n",
    "zz1 = F.softplus(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  1.,  1., -2.,  3.],\n",
      "        [ 1.,  0.,  1.,  4.,  4.,  4.]])\n",
      "tensor([[ 1.,  0.,  1.,  1.,  0.,  3.],\n",
      "        [ 1.,  0.,  1.,  4.,  4.,  4.]])\n",
      "tensor([ 2.,  0.,  2.,  5.,  4.,  7.])\n",
      "tensor([ 2.1269,  0.6931,  2.1269,  5.0067,  4.0181,  7.0009])\n"
     ]
    }
   ],
   "source": [
    "# print(x)\n",
    "print(z)\n",
    "print(F.relu(z))\n",
    "print(zz)\n",
    "print(zz1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as distributions\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BSG_Net import *\n",
    "import pickle\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/bsg-300.model', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "create_vocabulary('../data/processed/english-french_large/training-300.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divergence_closed_form(mu_1, sigma_1, mu_2, sigma_2):\n",
    "    posterior = distributions.MultivariateNormal(mu_1, torch.diag(sigma_1 ** 2))\n",
    "    prior = distributions.MultivariateNormal(mu_2, torch.diag(sigma_2 **2))\n",
    "    kl_z = torch.distributions.kl.kl_divergence(posterior, prior)\n",
    "\n",
    "    return kl_z\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entailment recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL\n",
      "5.876708984375\n"
     ]
    }
   ],
   "source": [
    "one = one_hot('senate')\n",
    "two = one_hot('senator')\n",
    "\n",
    "mean_one = model.p_mean(one)\n",
    "sigma_one = model.p_sigma(one)\n",
    "\n",
    "mean_two = model.p_mean(two)\n",
    "sigma_two = model.p_sigma(two)\n",
    "\n",
    "print('KL')\n",
    "print(divergence_closed_form(mean_one, sigma_one, mean_two, sigma_two).item())\n",
    "\n",
    "# print('cosine similarity')\n",
    "# sim = torch.dot(mean_one, mean_two) / (torch.norm(mean_one) * torch.norm(mean_two))\n",
    "# print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive \n",
    "\n",
    "one = one_hot('committee')\n",
    "two = one_hot('commission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative examples\n",
    "\n",
    "one = one_hot('senate')\n",
    "two = one_hot('senator')\n",
    "\n",
    "one = one_hot('speaker')\n",
    "two = one_hot('1950')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "a = torch.tensor([1.0, 2.0])\n",
    "\n",
    "print(F.softmax(a, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Substitution Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'side.n\t301\t29\ton sunday at craven cottage , jose mourinho and his all stars exhibited all of the above symptoms and they were made to pay the price by a fulham side that had in previous weeks woken up after matches with their heads kicked in .'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_words(center_position, sentence, context_size):\n",
    "    '''\n",
    "    Return a list of context words for the given center word\n",
    "\n",
    "    @param center_position position of center word in the sentence\n",
    "    @param sentence\n",
    "    @param context_size\n",
    "    @return context_words: list of words\n",
    "    '''\n",
    "\n",
    "    words = sentence.split()\n",
    "\n",
    "    context_words = []\n",
    "    for j in range(max(0, center_position - context_size), min(len(words), center_position + context_size + 1)):\n",
    "        if j == center_position:  # center_word is included in this range, ignore that\n",
    "            continue\n",
    "        context_words.append(words[j])\n",
    "\n",
    "    return context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 2\n",
    "sent = 'on our side : provide more aid , untied to trade ; write off debt ; help with good governance and infrastructure ; training to the soldiers , with un blessing , in conflict resolution ; encouraging investment ; and access to our markets so that we practise the free trade we are so fond of preaching .'\n",
    "\n",
    "get_context_words(pos, sent, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/english-french_large/training-300.en', 'r') as f:\n",
    "    data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "create_vocabulary('../data/processed/english-french_large/training-300.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_w2i.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'.' in global_w2i.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'1': '2'}\n",
    "\n",
    "for k, v in a.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
