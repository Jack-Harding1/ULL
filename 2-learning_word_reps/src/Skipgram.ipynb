{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevant import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import generate_training_set()\n",
    "\n",
    "def read_in_data(text_file):\n",
    "\n",
    "    data_output = []\n",
    "    with open(text_file, \"r\") as file1:\n",
    "        data_list = file1.readlines()\n",
    "    for line in data_list:\n",
    "        token_list = line.split()\n",
    "        data_output.append(token_list)\n",
    "    \n",
    "    return data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_in_data(\"../data/training.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(training_set):\n",
    "\n",
    "    vocabulary = []\n",
    "    for sentence in training_set:\n",
    "        for word in sentence:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "                \n",
    "    w2i = dict()\n",
    "    i2w = dict()\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        i2w[idx] = word\n",
    "        w2i[word] = idx\n",
    "    \n",
    "    return w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, i2w = create_vocabulary(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(w2i.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the skipgram data for a single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram(sentence, context_window_size):\n",
    "\n",
    "    skipgram_array = []\n",
    "    for idx, word in enumerate(sentence):\n",
    "        window_size = random.randint(1, context_window_size)\n",
    "        for index in range(max(idx - window_size, 0), min(len(sentence), idx + window_size + 1)):\n",
    "            if index != idx:\n",
    "                skipgram_array.append([w2i[word], w2i[sentence[index]]])\n",
    "\n",
    "    return skipgram_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the data for the whole training corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_skipgrams(training_set, window_size):\n",
    "\n",
    "    skipgrams = []\n",
    "    for sentence in training_set:\n",
    "        for skipgram in generate_skipgram(sentence, window_size):\n",
    "            skipgrams.append(skipgram)\n",
    "    random.shuffle(skipgrams)\n",
    "\n",
    "    return np.array(skipgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network which will generate the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dimension, vocabulary_size):\n",
    "        super(Skipgram_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "        self.fc2 = nn.Linear(embedding_dimension, vocabulary_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(skipgram_training_data, batch_size):\n",
    "\n",
    "    new_data = []\n",
    "    num_samples = skipgram_training_data.shape[0]\n",
    "    for idx in range(num_samples // batch_size):\n",
    "        batch = skipgram_training_data[(idx)*batch_size : (idx+1)*batch_size]\n",
    "        new_data.append(batch)\n",
    "    \n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "skipgram_training_data = generate_corpus_skipgrams(training_data, 5)\n",
    "skipgram_training_data = make_batches(skipgram_training_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 100\n",
    "epochs = 3\n",
    "\n",
    "vocab_size = len(w2i.keys())\n",
    "model = Skipgram_Net(EMBEDDING_DIMENSION, vocab_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "print(len(skipgram_training_data))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"EPOCH NUMBER:\", epoch)\n",
    "    i = 0\n",
    "    \n",
    "    for data_point in skipgram_training_data:\n",
    "        x_values = data_point[:, 0]\n",
    "        y_values = data_point[:, 1]\n",
    "        input_to_network = torch.zeros(BATCH_SIZE, vocab_size)\n",
    "\n",
    "        for idx in range(BATCH_SIZE):\n",
    "            input_to_network[idx, x_values[idx]] = 1.0\n",
    "        target = torch.tensor(y_values, dtype=torch.long)\n",
    "        output_of_network = model(input_to_network)\n",
    "        \n",
    "        loss = loss_fn(output_of_network, target)\n",
    "        if loss.item() < 0.01:\n",
    "            break\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"LOSS at step {} was {}\".format(i, loss.item()))\n",
    "        i +=1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "\n",
    "# st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "# create directory if it does not exist\n",
    "if not os.path.exists('../models'):\n",
    "    os.makedirs('../models')\n",
    "\n",
    "with open('../models/skipgram_{}-{}.model'.format(str(epochs), str(EMBEDDING_DIMENSION)), 'wb') as f:\n",
    "    pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the word embeddings from the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(trained_model):\n",
    "\n",
    "    embeddings = dict()\n",
    "    params = list(trained_model.parameters())\n",
    "    learned_weights = trained_model.fc1.weight.data\n",
    "    \n",
    "    for word in w2i.keys():\n",
    "        word_idx = w2i[word]\n",
    "        embeddings[word] = learned_weights[:, word_idx].numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_filepath = '../models/embeddings.pickle'\n",
    "\n",
    "# check if embeddings file exists\n",
    "if os.path.exists(embeddings_filepath):\n",
    "    with open(embeddings_filepath, 'rb') as file:\n",
    "        embeddings_dict = pickle.load(file)\n",
    "else:\n",
    "    embeddings_dict = create_embeddings(model)\n",
    "    with open(embeddings_filepath, 'wb') as file:\n",
    "        pickle.dump(embeddings_dict, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_dict[\"the\"])\n",
    "print(embeddings_dict[\"as\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
