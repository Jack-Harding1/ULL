{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import generate_training_set()\n",
    "\n",
    "def read_in_data(text_file):\n",
    "    data_output = []\n",
    "    with open(text_file, \"r\") as file1:\n",
    "        data_list = file1.readlines()\n",
    "    for line in data_list:\n",
    "        token_list = line.split()\n",
    "        data_output.append(token_list)\n",
    "    return data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_in_data(\"training.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(training_set):\n",
    "    vocabulary = []\n",
    "    for sentence in training_set:\n",
    "        for word in sentence:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "                \n",
    "    w2i = dict()\n",
    "    i2w = dict()\n",
    "    for idx, word in enumerate(vocabulary):\n",
    "        i2w[idx] = word\n",
    "        w2i[word] = idx\n",
    "    \n",
    "    return w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, i2w = create_vocabulary(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36635\n"
     ]
    }
   ],
   "source": [
    "print(len(w2i.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the skipgram data for a single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram(sentence, context_window_size):\n",
    "    skipgram_array = []\n",
    "    for idx, word in enumerate(sentence):\n",
    "        window_size = random.randint(1, context_window_size)\n",
    "        for index in range(max(idx - window_size, 0), min(len(sentence), idx + window_size + 1)):\n",
    "            if index == idx:\n",
    "                pass\n",
    "            else:\n",
    "                skipgram_array.append([w2i[word], w2i[sentence[index]]])\n",
    "    return skipgram_array\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the data for the whole training corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus_skipgrams(training_set, window_size):\n",
    "    skipgrams = []\n",
    "    for sentence in training_set:\n",
    "        for skipgram in generate_skipgram(sentence, window_size):\n",
    "            skipgrams.append(skipgram)\n",
    "    random.shuffle(skipgrams)\n",
    "    return np.array(skipgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the network which will generate the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram_Net(nn.Module):\n",
    "    def __init__(self, embedding_dimension, vocabulary_size):\n",
    "        super(Skipgram_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocabulary_size, embedding_dimension)\n",
    "        self.fc2 = nn.Linear(embedding_dimension, vocabulary_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(skipgram_training_data, batch_size):\n",
    "    new_data = []\n",
    "    num_samples = skipgram_training_data.shape[0]\n",
    "    for idx in range(num_samples // batch_size):\n",
    "        batch = skipgram_training_data[(idx)*batch_size : (idx+1)*batch_size]\n",
    "        new_data.append(batch)\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "skipgram_training_data = generate_corpus_skipgrams(training_data, 5)\n",
    "skipgram_training_data = make_batches(skipgram_training_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204138\n",
      "EPOCH NUMBER: 0\n",
      "LOSS at step 0 was 10.529213905334473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS at step 100 was 10.497106552124023\n",
      "LOSS at step 200 was 10.426531791687012\n",
      "LOSS at step 300 was 10.264270782470703\n",
      "LOSS at step 400 was 10.031286239624023\n",
      "LOSS at step 500 was 9.665971755981445\n",
      "LOSS at step 600 was 9.226372718811035\n",
      "LOSS at step 700 was 8.732582092285156\n",
      "LOSS at step 800 was 8.236682891845703\n",
      "LOSS at step 900 was 7.920852184295654\n",
      "LOSS at step 1000 was 7.865729808807373\n",
      "LOSS at step 1100 was 7.284582614898682\n",
      "LOSS at step 1200 was 7.542508602142334\n",
      "LOSS at step 1300 was 6.8805742263793945\n",
      "LOSS at step 1400 was 6.900568962097168\n",
      "LOSS at step 1500 was 6.9012556076049805\n",
      "LOSS at step 1600 was 6.728001594543457\n",
      "LOSS at step 1700 was 6.968002796173096\n",
      "LOSS at step 1800 was 6.4681243896484375\n",
      "LOSS at step 1900 was 6.379810810089111\n",
      "LOSS at step 2000 was 6.505972385406494\n",
      "LOSS at step 2100 was 6.445388317108154\n",
      "LOSS at step 2200 was 6.7486796379089355\n",
      "LOSS at step 2300 was 7.121939659118652\n",
      "LOSS at step 2400 was 6.540307521820068\n",
      "LOSS at step 2500 was 7.535333156585693\n",
      "LOSS at step 2600 was 6.667308330535889\n",
      "LOSS at step 2700 was 6.671991348266602\n",
      "LOSS at step 2800 was 6.9689412117004395\n",
      "LOSS at step 2900 was 6.337568759918213\n",
      "LOSS at step 3000 was 7.338585376739502\n",
      "LOSS at step 3100 was 6.420426845550537\n",
      "LOSS at step 3200 was 6.8064799308776855\n",
      "LOSS at step 3300 was 6.810361862182617\n",
      "LOSS at step 3400 was 6.806645393371582\n",
      "LOSS at step 3500 was 6.5367960929870605\n",
      "LOSS at step 3600 was 6.548516273498535\n",
      "LOSS at step 3700 was 6.7890849113464355\n",
      "LOSS at step 3800 was 7.0446038246154785\n",
      "LOSS at step 3900 was 6.583132266998291\n",
      "LOSS at step 4000 was 6.43505859375\n",
      "LOSS at step 4100 was 6.893194675445557\n",
      "LOSS at step 4200 was 6.520237922668457\n",
      "LOSS at step 4300 was 6.500131130218506\n",
      "LOSS at step 4400 was 6.512266635894775\n",
      "LOSS at step 4500 was 6.815526008605957\n",
      "LOSS at step 4600 was 6.787583827972412\n",
      "LOSS at step 4700 was 6.544909477233887\n",
      "LOSS at step 4800 was 6.264136791229248\n",
      "LOSS at step 4900 was 6.719597816467285\n",
      "LOSS at step 5000 was 7.173706531524658\n",
      "LOSS at step 5100 was 6.465566635131836\n",
      "LOSS at step 5200 was 6.888487339019775\n",
      "LOSS at step 5300 was 6.597411632537842\n",
      "LOSS at step 5400 was 6.7200469970703125\n",
      "LOSS at step 5500 was 6.288331508636475\n",
      "LOSS at step 5600 was 6.412874698638916\n",
      "LOSS at step 5700 was 6.4434309005737305\n",
      "LOSS at step 5800 was 6.466903209686279\n",
      "LOSS at step 5900 was 6.600217819213867\n",
      "LOSS at step 6000 was 6.855445384979248\n",
      "LOSS at step 6100 was 6.252969264984131\n",
      "LOSS at step 6200 was 6.97000789642334\n",
      "LOSS at step 6300 was 6.510039806365967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7bca4d61df4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION = 100\n",
    "vocab_size = len(w2i.keys())\n",
    "model = Skipgram_Net(EMBEDDING_DIMENSION, vocab_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "print(len(skipgram_training_data))\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(\"EPOCH NUMBER:\", epoch)\n",
    "    i = 0\n",
    "    for data_point in skipgram_training_data:\n",
    "        x_values = data_point[:,0]\n",
    "        y_values = data_point[:,1]\n",
    "        input_to_network = torch.zeros(BATCH_SIZE, vocab_size)\n",
    "        for idx in range(BATCH_SIZE):\n",
    "            input_to_network[idx, x_values[idx]] = 1.0\n",
    "        target = torch.tensor(y_values, dtype=torch.long)\n",
    "        output_of_network = model(input_to_network)\n",
    "        \n",
    "        loss = loss_fn(output_of_network, target)\n",
    "        if loss.item() < 0.01:\n",
    "            break\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"LOSS at step {} was {}\".format(i, loss.item()))\n",
    "        i +=1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the word embeddings from the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(trained_model):\n",
    "    embeddings = dict()\n",
    "    params = list(trained_model.parameters())\n",
    "    learned_weights = trained_model.fc1.weight.data\n",
    "    for word in w2i.keys():\n",
    "        word_idx = w2i[word]\n",
    "        embeddings[word] = learned_weights[:, word_idx].numpy()\n",
    "    return embeddings\n",
    "\n",
    "embeddings_dict = create_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00928869 -0.0232761   0.02100292 -0.00405158 -0.0309203   0.02540967\n",
      " -0.00217791  0.02410931  0.05206803 -0.01831422 -0.05881453  0.01429875\n",
      " -0.00822011  0.01345269  0.009621    0.00642681  0.03871971 -0.00523754\n",
      "  0.04309646  0.03745118 -0.00880113  0.05257478 -0.04627841  0.04463107\n",
      "  0.00845229 -0.02368473 -0.00888878 -0.00955483  0.03948393 -0.02397909\n",
      "  0.02089704  0.01606143  0.02256639  0.01473693  0.05186039 -0.04609038\n",
      "  0.01349395 -0.02449494 -0.00146349 -0.0455589  -0.06575637  0.00218241\n",
      " -0.00816757  0.02345874  0.00545223 -0.03933325 -0.02481946  0.00852639\n",
      " -0.01204512  0.0238484  -0.00201765  0.05104383  0.00889562  0.0027562\n",
      " -0.02173823  0.0430742  -0.02392623  0.00160262  0.02351004 -0.03623751\n",
      " -0.05046012  0.0224936   0.01142139 -0.00485048 -0.042598    0.04487596\n",
      " -0.00352428 -0.00651073 -0.01516988 -0.04971393  0.00345578  0.01938071\n",
      "  0.00084703 -0.00105783  0.02925267  0.01949517 -0.05746623  0.05075869\n",
      " -0.00186497 -0.02196809 -0.01765833  0.00055218 -0.05511438  0.03928856\n",
      " -0.05187961 -0.02096514  0.0462612  -0.02495339 -0.01534559 -0.00606144\n",
      "  0.02597544 -0.06087507  0.04783637  0.05555474 -0.05457233 -0.00369836\n",
      "  0.06059451  0.05918329 -0.00271161  0.0007257 ]\n",
      "[-0.03592218 -0.03150701  0.02500179 -0.02977924 -0.00264772  0.04465818\n",
      " -0.0265946   0.02010848  0.02103466 -0.04036526 -0.03601306  0.01537736\n",
      " -0.03251988  0.0126211   0.03582547  0.03445894  0.00570364 -0.00190481\n",
      "  0.02554615  0.04094513 -0.00471251  0.02778784 -0.02490074  0.00393486\n",
      " -0.01807708 -0.01368281  0.01764613  0.01781827  0.02996514  0.01724547\n",
      " -0.01853587 -0.03723589  0.01102076  0.0003507   0.02435936 -0.01970938\n",
      " -0.0203071  -0.00054518 -0.0137136  -0.01221652 -0.01639325 -0.04256758\n",
      " -0.02529625  0.03185816 -0.02924717 -0.02132284 -0.01900894 -0.01619133\n",
      " -0.01824453  0.02304526  0.01323377  0.00126906 -0.01186135 -0.02076163\n",
      "  0.0325604   0.02072847 -0.00119591  0.02684422  0.04830007 -0.02989363\n",
      " -0.01945492  0.03308716  0.01459729  0.02856801 -0.01747169  0.02904618\n",
      " -0.02564096 -0.02552658 -0.0289855  -0.032625    0.02967627  0.01813681\n",
      "  0.0352911  -0.03178049  0.01455809  0.01848735 -0.00776723  0.00508491\n",
      "  0.0074174   0.01005565  0.02683432 -0.01606357 -0.00211423 -0.03097017\n",
      " -0.01891047 -0.01755589 -0.00182911 -0.02681316  0.02355736 -0.02361214\n",
      "  0.01916898 -0.00838152  0.02831152 -0.00421367 -0.00804118  0.01271483\n",
      "  0.02595081  0.03588109  0.0306452  -0.04896777]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dict[\"the\"])\n",
    "print(embeddings_dict[\"as\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
